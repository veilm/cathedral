# Situational Awareness Wiki

This wiki consolidates Leopold Aschenbrenner's "Situational Awareness: The Decade Ahead" essay series (June 2024), which argues that AGI is likely by 2027 and superintelligence by the end of the decade, with profound implications for national security, safety, and the future of civilization.

## Core Predictions and Arguments

**Timeline and Capabilities**
- [[agi-timeline]] - AGI by 2027 via continued scaling trends
- [[counting-the-ooms]] - Methodology for predicting AI progress through orders of magnitude
- [[intelligence-explosion]] - Rapid transition from AGI to superintelligence in <1 year
- [[superintelligence]] - Vastly superhuman AI systems by end of decade
- [[automated-ai-research]] - How AGI automates its own development

**Technical Foundations**
- [[gpt-models]] - Historical progression from GPT-2 to GPT-4 as empirical basis
- [[algorithmic-efficiency]] - Compute multipliers driving ~half of AI progress
- [[unhobbling]] - Unlocking latent capabilities to transform chatbots into agents
- [[data-wall]] - Challenge of running out of training data and proposed solutions

**Infrastructure and Economics**
- [[training-clusters]] - Scaling from $500M to trillion-dollar compute clusters
- [[ai-investment]] - Investment trajectory from $150B (2024) to $8T (2030)
- [[algorithmic-secrets]] - Most valuable and vulnerable national security assets

## National Security and Geopolitics

**The Competition**
- [[china-competition]] - How China can compete through outbuilding and theft
- [[decisive-military-advantage]] - Why superintelligence confers complete dominance
- [[ai-lab-security]] - Catastrophic failures in protecting AGI secrets and weights

**The Response**
- [[the-project]] - Inevitable government Manhattan Project for AGI by 2026-2028
- [[nonproliferation-regime]] - US-led effort to stabilize post-superintelligence world

## Safety and Alignment

- [[superalignment-problem]] - Challenge of controlling AI systems smarter than humans
- [[situational-awareness]] - Understanding what is actually being built (possessed by few hundred people)

## Key Figures

- [[leopold-aschenbrenner]] - Author, former OpenAI superalignment researcher

## Conceptual Framework

The series argues for "AGI Realism" between doomer and e/acc extremes, with three core tenets: (1) Superintelligence is a matter of national security, (2) America must lead, and (3) We need to not screw it up. The window for action is narrowâ€”the next 12-24 months are critical for locking down algorithmic secrets, and the next few years will determine whether humanity successfully navigates to superintelligence or faces catastrophic failure modes ranging from authoritarian dominance to self-destruction.
