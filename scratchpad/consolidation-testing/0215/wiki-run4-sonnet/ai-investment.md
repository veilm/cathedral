# AI Investment

AI investment is scaling at unprecedented rates, with total annual investment projected to grow from ~$150B (2024) to potentially $8T (2030), representing one of the largest capital buildouts in history.

## 2024 Baseline

**Nvidia datacenter revenue**: ~$90B annualized (from ~$14B year prior)
- But Nvidia isn't only player (Google TPUs, etc.)
- ~50-60% of datacenter capex is non-chip costs

**Big tech capex ramp**:
- Microsoft and Google: $50B+ each
- AWS and Meta: $40B+ each
- Year-over-year growth: $50-100B increase due to AI boom
- Still cutting other capex to shift more to AI
- Tesla alone: $10B on AI this year

**Total 2024**: ~$150B AI investment
- 5-10M H100-equivalents
- 1-2% of US electricity production
- 5-10% of TSMC leading-edge capacity

## Revenue Driving Investment

**OpenAI trajectory** (extrapolatable):
- $1B annual run rate: August 2023
- $2B annual run rate: February 2024
- Roughly doubling every 6 months
- Projected $10B+ run rate: late 2024/early 2025

**Microsoft AI revenue**: Estimated ~$5B incremental already

**Key milestone**: When will big tech hit $100B revenue run rate from AI?
- Naive extrapolation: mid-2026
- Would represent substantial fraction of $100-300B total revenues
- Example path: 1/3 of Microsoft's 350M Office subscribers paying $100/month for AI add-on
- Only needs few hours/month productivity gain to justify

## Investment Justification Logic

**So far**: Every 10x scaleup yields necessary returns
- GPT-3.5: Unleashed ChatGPT mania
- GPT-4 cluster (~$500M): Paid off by billions in annual revenue
- 2024-class cluster (billions): Will pay off if revenue hits $10B+ run rate

**Investment-led boom**: Takes years from GPU order → cluster built → models trained → rolled out. Clusters being planned today are years out. But if returns on last order materialize, investment continues skyrocketing.

## Projected Growth

Total world AI investment growing ~2x/year:

**2026**: ~$500B
- 10s of millions of H100-equivalents
- 5% of US electricity
- ~25% of TSMC leading-edge capacity

**2028**: ~$2T
- 100 million H100-equivalents
- 20% of US electricity
- ~100% of current TSMC capacity

**2030**: ~$8T
- Many 100s of millions of H100-equivalents
- 100% of US electricity (or 4x current capacity elsewhere)
- 4x current TSMC capacity needed

## Historical Precedents

**Not unprecedented scale**:
- Manhattan/Apollo programs: 0.4% of GDP in peak years (~$100B today)
- At $1T/year, AI would be ~3% of GDP
- Telecoms (1996-2001): Nearly $1T (today's dollars) on internet infrastructure
- British railways (1841-1850): ~40% of British GDP over decade (~$11T equivalent for US today)
- Green transition: Many trillions being spent
- China investment: >40% of GDP for two decades (~$11T annually at US GDP)
- WWI/WWII: Countries borrowed >100% of GDP for war efforts (>$17T US equivalent for WWII)

$1T/year by 2027 would be "among the very largest capital buildouts ever—but would not be unprecedented."

## Breakdown: Training vs. Inference

Large fraction (eventually majority) used for inference rather than training:
- Meta example: Only ~45k of 350k H100s in largest training clusters
- Rest for inference (running models for products)
- As AI products scale, inference becomes dominant

## Market Forecasts

**AMD forecast** (2023): $400B AI accelerator market by 2027
- Implies $700B+ total AI spending
- "Pretty close to my numbers (and they are surely much less 'AGI-pilled' than I am)"

**Sam Altman reported talks**: "Up to $7T" in capex for AI compute capacity
- "Widely mocked, but it seems less crazy if you run the numbers"

## Economic Implications

**First $10T company**: Likely soon after big tech hits $100B AI revenue run rate
- Stock markets would follow forecasts of overall revenue growth
- AI becoming biggest revenue driver and growth area

**First many-hundred-billion dollar corporate bond sale**: Expected around this milestone

**Interest rate impacts**: Uncertain but significant (See Tyler Cowen, Chow/Mazlish/Halperin analyses)

## Critical Constraint

Not willingness-to-spend but finding infrastructure:
- "Where do I find 10GW?" (for 2028 $100B+ cluster)
- GPU wait times: ~1 year
- Power/land/permitting lead times: Much longer

"Perhaps the wildest part is that willingness-to-spend doesn't even seem to be the binding constraint at the moment, at least for training clusters."

## See Also
- [[training-clusters]]
- [[power-buildout]]
- [[counting-the-ooms]]
- [[ai-revenue]]
- [[the-project]]
