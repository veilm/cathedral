# Leopold Aschenbrenner

Leopold Aschenbrenner is the author of "Situational Awareness: The Decade Ahead" (June 2024), a former OpenAI researcher who worked on superalignment, and one of the few hundred people with deep [[situational-awareness]] about the coming transformation.

## Background

**Former OpenAI affiliation**: "While I used to work at OpenAI, all of this is based on publicly-available information, my own ideas, general field-knowledge, or SF-gossip."

**Work focus**: Spent past year working on technical research on aligning AI systems as day-job at OpenAI, working with Ilya Sutskever and the Superalignment team.

**Academic background**: Previously did research in economics, including work on semi-endogenous growth theory and technological progress models.

## Perspective

**Not a doomer**: "Certified by the doomer-in-chief" (Yudkowsky). "Strong optimist that this problem is solvable." Spent considerable energies debating AI pessimists and strongly advocated against policies like AI pause.

**Primary concerns**: "Most worried about things just being totally crazy around superintelligence, including things like novel WMDs, destructive wars, and unknown unknowns." Also: "Arc of history counsels us to not underrate authoritarianism—and superintelligence might allow authoritarians to dominate for billions of years."

**On muddling through**: "Tyler Cowen says, muddling through is underrated!" Believes in empirical approach and leveraging what's working in deep learning.

## Acknowledgments

Essay series thanked:
- Collin Burns, Avital Balwit, Carl Shulman, Jan Leike, Ilya Sutskever
- Holden Karnofsky, Sholto Douglas, James Bradbury, Dwarkesh Patel
- Many friends for feedback on earlier drafts
- Joe Ronan for graphics, Nick Whitaker for publishing help
- "Dedicated to Ilya Sutskever"

## Key Predictions

**AGI timeline**: 2027 is "strikingly plausible" - systems capable of automating AI research and most cognitive jobs

**Intelligence explosion**: Less than 1 year from AGI to vastly superhuman superintelligence via automated research

**Government involvement**: By 2026-2028, [[the-project|Manhattan Project-style]] government AGI effort inevitable

**International competition**: Full-throated [[china-competition|CCP AGI effort]] expected once they "wake up" to AGI

**Critical failures**: Without improved [[ai-lab-security|security]], "in next 12-24 months, we will leak key AGI breakthroughs to the CCP"

## Methodology

**Counting the OOMs**: Core approach is extrapolating from measurable trendlines in compute (~0.5 OOMs/year), algorithmic efficiency (~0.5 OOMs/year), and unhobbling gains.

**Not scifi belief**: "Doesn't require believing in sci-fi; it just requires believing in straight lines on a graph."

**Empirical grounding**: Heavy use of specific numbers, dates, estimates from public sources (Epoch AI, company reports, research papers)

## Advocacy Positions

**National security priority**: "AGI is an existential challenge for the national security of the United States. It's time to start treating it as such."

**Specific urgencies**:
1. Rapidly lock down AI labs (algorithmic secrets most urgent)
2. Build clusters in US, not dictatorships
3. AI labs must work with intelligence community and military
4. Immediate security crash program needed

**On balance of risks**: "Most of all, ensuring alignment doesn't go awry will require extreme competence in managing the intelligence explosion."

## Personal Stakes

**Investment**: "Sure, going all-in leveraged long Nvidia in early 2023 has been great and all"

**Burden**: "The burdens of history are heavy. I would not choose this."

**Duty**: "For those of us who get the call to come along for the ride, it'll be...stressful. But it will be our duty to serve the free world—and all of humanity."

## Publication Details

**Format**: Essay series on situational-awareness.ai, June 2024
- Available as PDF
- Featured on Dwarkesh Patel podcast
- Individual essays designed to stand alone

**Series structure**:
1. From GPT-4 to AGI: Counting the OOMs
2. From AGI to Superintelligence: the Intelligence Explosion
3. The Challenges (4 parts: Clusters, Security, Alignment, Geopolitics)
4. The Project
5. Parting Thoughts

## See Also
- [[situational-awareness]]
- [[agi-timeline]]
- [[counting-the-ooms]]
- [[the-project]]
- [[agi-realism]]
