# Training Clusters

The largest AI training clusters are scaling at ~0.5 OOMs/year (3x-5x annually), requiring extraordinary industrial mobilization for power and chips.

## Historical Scale

**GPT-4 (2022)**:
- ~10,000 H100-equivalents (~25k A100s)
- Cost: ~$500 million
- Power: ~10 MW
- Comparable to powering 10,000 average homes

## Projected Scale

**2024 cluster** (+1 OOM):
- ~100,000 H100-equivalents
- Cost: single-digit billions
- Power: ~100 MW
- Reference: 100,000 homes, or large nuclear reactor

**2026 cluster** (+2 OOMs):
- ~1 million H100-equivalents
- Cost: $10s of billions
- Power: ~1 GW
- Reference: Hoover Dam capacity
- Example: Kuwait reportedly building 1GW, 1.4M H100-equivalent cluster

**2028 cluster** (+3 OOMs):
- ~10 million H100-equivalents
- Cost: $100s of billions (comparable to International Space Station)
- Power: ~10 GW
- Reference: Small/medium US state electricity consumption
- Example: Microsoft/OpenAI rumored $100B cluster

**2030 trillion-dollar cluster** (+4 OOMs):
- ~100 million H100-equivalents
- Cost: $1+ trillion
- Power: ~100 GW
- Reference: >20% of total US electricity production
- Would require hundreds of power plants

## Overall Compute Investment

Beyond single training clusters, total AI investment grows ~2x/year:

- **2024**: ~$150B (5-10M H100-equivalents, 1-2% US electricity)
- **2026**: ~$500B (10s of millions chips, 5% US electricity, ~25% TSMC capacity)
- **2028**: ~$2T (100M chips, 20% US electricity, ~100% TSMC capacity)
- **2030**: ~$8T (100s of millions chips, 100% US electricity, 4x current TSMC)

Large fraction (perhaps majority) of chips used for inference rather than training.

## Power Constraints

**Current**: US electricity production barely grown 5% in last decade

**Solution - Natural Gas**:
- Marcellus/Utica shale alone produces enough gas for 150-250 GW continuous
- Would require ~1,200 new wells for 100GW cluster
- 40 rigs could build production base in <1 year
- Combined cycle plants buildable in ~2 years
- ~$100B capex for 100GW power plants

The barriers are primarily regulatory (climate commitments, permitting, NEPA, FERC) rather than technical.

## Chip Constraints

**Current production**: AI chips ~5-10% of TSMC leading-edge capacity (2024)

**2030 needs**: Would require multiple TSMC "gigafabs" at ~$20B each, plus massive HBM memory and CoWoS packaging buildout. Total capex potentially >$1T.

TSMC currently forecasts only 50% CAGR for AI (vastly underestimating coming demand).

## National Security Imperative

Clusters must be built in US or close democratic allies, not Middle Eastern dictatorships offering easy money. Physical location of AGI datacenters more critical than chip fab locationsâ€”having the datacenter abroad is "like having the literal nukes be built and stored abroad."

## See Also
- [[counting-the-ooms]]
- [[ai-investment]]
- [[power-buildout]]
- [[the-project]]
