# GPT Models

The GPT series illustrates the rapid progression of AI capabilities from 2019-2023, serving as the empirical foundation for AGI timeline projections.

## GPT-2 (2019): Preschooler Level

Could occasionally string together a few plausible sentences in cherry-picked examples. A semi-coherent story about unicorns in the Andes was "incredibly impressive at the time."

**Limitations**:
- Barely count to 5 without getting tripped up
- Summarizing: just barely outperformed selecting 3 random sentences
- What impressed: occasionally answering simple factual questions correctly

**Human analogy**: Impressive for command of language and occasionally generating semi-cohesive paragraph—"what would have been impressive for a preschooler."

## GPT-3 (2020): Elementary School Level

"Wow, with just some few-shot examples it can do some simple useful tasks."

**Capabilities**:
- Cohesive over multiple paragraphs more consistently
- Correct grammar
- Very basic arithmetic
- First commercial uses: simple SEO/marketing copy

**What impressed**:
- Using made-up words in new sentences
- Rich storytelling back-and-forth
- Generating very simple code
- Learning from simple instructions/demonstrations

**Human analogy**: Basic poetry, coherent stories, rudimentary coding—"what would have been impressive for an elementary schooler."

**Note**: This is "clunky old GPT-3," not the dramatically-improved GPT-3.5 from ChatGPT.

## GPT-3.5 (Late 2022): Breakthrough to Usability

Released less than year before GPT-4. On standardized tests: well below median human → top of human range in one year.

**Key innovation**: RLHF made models actually usable (ChatGPT revolution).

## GPT-4 (2023): Smart High Schooler Level

"It can write pretty sophisticated code and iteratively debug, write intelligently about complicated subjects, reason through difficult high-school competition math, beating vast majority of high schoolers on tests."

**Performance**:
- Scores better than vast majority of high schoolers on AP exams and SAT
- Can think and reason (code, math, Fermi estimates)
- Useful in daily tasks (writing code, revising drafts)
- Writes complicated code, reasons through nontrivial math

**Limitations**: Somewhat uneven across tasks. "Most limitations come down to obvious ways models are still hobbled" - raw intelligence mostly there even if artificially constrained.

**Key leap from GPT-3.5**: One year took us from well below median human to top of range on standardized tests.

## The 4-Year Jump

From GPT-2 (2019) to GPT-4 (2023): ~4 years
- Preschooler → Smart high schooler
- Barely coherent sentences → Acing college exams
- Simple factual questions → Sophisticated reasoning

This represents 4.5-6 OOMs base effective compute scaleup plus major unhobbling gains.

## What's Next

Projecting another 4 years (2023-2027) with similar drivers:
- 3-6 OOMs effective compute scaleup
- Step-changes from unhobbling (chatbot → agent/drop-in remote worker)
- "Another GPT-2-to-GPT-4-sized jump"

Would likely yield systems that can "outperform PhDs and best experts in a field" and automate cognitive jobs including AI research itself.

## Benchmarks Rapidly Falling

**MMLU (2020)**: Designed to "stand the test of time" with hardest high school/college exams
- Just 3 years later: basically solved (~90% for GPT-4/Gemini)

**MATH (2021)**: Difficult high school competition math
- Original paper: "need new algorithmic advancements...just scaling won't work"
- Researchers predicted minimal progress
- Reality: ~5% (2021) → 50% (2022) → >90% (2024)

**Current frontier**: GPQA (PhD-level biology/chemistry/physics)
- Claude 3 Opus: ~60% (in-domain PhDs: ~80%)
- "We'll probably crack expert-PhD-level soon"
- Questions "read like gibberish" to non-experts

**Pattern**: "Over and over again, year after year, skeptics have claimed 'deep learning won't be able to do X' and have been quickly proven wrong."

**Key lesson**: "If there's one lesson we've learned from the past decade of AI, it's that you should never bet against deep learning."

## See Also
- [[agi-timeline]]
- [[counting-the-ooms]]
- [[unhobbling]]
- [[benchmarks]]
