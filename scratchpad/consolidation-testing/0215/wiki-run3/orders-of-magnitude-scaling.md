# Orders of Magnitude Scaling

An order of magnitude (OOM) represents a 10x increase. OOM scaling is the fundamental framework for understanding and predicting AI progress in the essay series.

## The OOM Framework

- 3x increase = 0.5 OOMs
- 10x increase = 1 OOM
- 100x increase = 2 OOMs
- 1,000x increase = 3 OOMs
- 100,000x increase = 5 OOMs

## Historical Scaling Rates

From GPT-2 (2019) to GPT-4 (2023), the increase in effective compute was approximately 4.5-6 OOMs total, combining:
- Physical compute: ~3-4 OOMs (GPT-2 used ~4e21 FLOP, GPT-4 used 8e24 to 4e25 FLOP according to Epoch AI estimates)
- [[algorithmic-progress]]: ~1-2 OOMs over the same period
- [[unhobbling-gains]]: Major but harder to quantify on unified scale

## Trend Rates

The long-running trend shows:
- Training compute for frontier models: ~0.5 OOMs/year
- Algorithmic efficiencies: ~0.5 OOMs/year (based on ImageNet data from 2012-2021)
- Combined rate: Approximately 1 OOM/year of effective compute growth

## Future Projections

By 2027 (4 years from GPT-4's 2023 release):
- Physical compute: +2-3 OOMs (training clusters of $10s-$100s of billions)
- Algorithmic efficiencies: +1-3 OOMs
- Total: ~3-6 OOMs of base effective compute
- Plus major unhobbling gains (chatbot â†’ agent transformation)

This would represent another GPT-2-to-GPT-4-sized qualitative jump, potentially reaching [[agi-definition-and-timeline]] capability levels.

## "Racing Through the OOMs"

The current decade is special because progress through OOMs is uniquely rapid. Moore's Law historically provided only 1-1.5 OOMs per decade. The current pace of ~5 OOMs in 4 years represents roughly 5x the speed of Moore's Law at its heyday, driven by massive investment rather than mere hardware improvements.

See also: [[compute-scaling-laws]], [[effective-compute-growth]], [[algorithmic-progress]]
