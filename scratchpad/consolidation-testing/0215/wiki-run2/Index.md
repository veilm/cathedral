# Situational Awareness: The Decade Ahead

This wiki consolidates Leopold Aschenbrenner's essay series on AGI timelines, risks, and implications, published June 2024. The core thesis: AGI by 2027 is strikingly plausible through straightforward extrapolation of deep learning trends, followed by rapid intelligence explosion to superintelligence by 2028-2030.

## AGI Timeline and Technical Progress

**[[agi-timeline]]** - Central projection that AGI arrives by 2027 based on continuing historical trends, followed by rapid transition to superintelligence by 2028-2030.

**[[counting-the-ooms]]** - Framework for projecting AI progress by tracking orders of magnitude growth in effective compute across three dimensions: physical compute, algorithmic efficiencies, and unhobbling gains.

**[[unhobbling]]** - Algorithmic improvements that unlock latent capabilities by removing artificial limitations, transforming chatbots into autonomous agents capable of independent work.

## Intelligence Explosion and Superintelligence

**[[intelligence-explosion]]** - The recursive improvement process where AI systems automate AI research itself, compressing decades of progress into ~1 year and rapidly ascending from human-level to vastly superhuman systems.

**[[superintelligence]]** - AI systems vastly smarter than humans in qualitative, not just quantitative, ways. Expected by 2028-2030, capable of transforming civilization through accelerated R&D, economic explosion, and military revolution.

## Infrastructure and Industrial Mobilization

**[[trillion-dollar-cluster]]** - The massive compute infrastructure required for AGI and superintelligence, scaling from $500M (GPT-4) to $1T+ clusters requiring >20% of US electricity production by 2030.

## Security Challenges

**[[ai-security]]** - The critical problem of protecting model weights and algorithmic secrets from state-actor theft. Current security at AI labs is woefully inadequate; without drastic improvements in next 12-24 months, key AGI breakthroughs will irreversibly leak to China.

**[[china-agi-competition]]** - Analysis of China's path to competitiveness through outbuilding US infrastructure and stealing algorithmic secrets, despite chip export controls.

## Safety and Alignment

**[[superalignment]]** - The technical problem of reliably controlling AI systems much smarter than humans. Current techniques (RLHF) fundamentally won't scale to superintelligence; the intelligence explosion makes this problem incredibly tense with little margin for error.

## Geopolitical and Military Implications

**[[military-advantage-from-superintelligence]]** - Why superintelligence will provide decisive military advantage comparable to nuclear weapons, enabling complete dominance including ability to overcome nuclear deterrents.

**[[the-project]]** - The inevitable US government-led effort to develop superintelligence (expected by 2026-2028), analogous to the Manhattan Project, necessitated by national security imperatives too great for private startups to handle.

## Key Concepts and Mechanisms

### Technical Concepts
- **OOM (Order of Magnitude)**: 10x increase. Framework for tracking cumulative progress across multiple dimensions (compute, algorithms, unhobbling).
- **Effective Compute**: Combined measure of physical compute × algorithmic efficiencies × unhobbling gains.
- **Test-Time Compute**: Computation used during inference/reasoning vs. training. Massive "overhang" in potential gains from enabling longer-horizon thinking.
- **Data Wall**: Exhaustion of internet training data creating potential bottleneck, requiring new paradigms (synthetic data, RL, self-play).

### Alignment Concepts
- **RLHF (Reinforcement Learning from Human Feedback)**: Current primary alignment technique—works for sub-human systems but breaks down for superintelligence.
- **Scalable Oversight**: Using AI assistants to help humans supervise other AI systems beyond human individual capability.
- **Weak-to-Strong Generalization**: Studying whether weak supervisor (e.g., GPT-2) can align strong model (e.g., GPT-4).
- **Sideconstraints**: Basic behavioral restrictions like "don't lie," "follow the law" that we need to reliably instill.

### Strategic Concepts
- **Situational Awareness**: Understanding what we're building and the implications—currently possessed by perhaps "a few hundred people" in San Francisco and AI labs.
- **AGI Realism**: Third way between doomers and e/accs: recognizing superintelligence as matter of national security requiring American leadership without AI pause.
- **Nonproliferation Regime**: Using American lead to establish safety norms and prevent proliferation to rogue actors, analogous to nuclear nonproliferation treaties.

## Critical Timelines

- **2024**: Current moment. Algorithmic secrets being developed now. Security must improve in next 12-24 months to prevent irreversible leaks.
- **2025-2026**: "Shocking" capability leaps expected. $100B annual AI revenues. Consensus begins forming on AGI proximity. Government starts waking up.
- **2026-2027**: "The moment" when it becomes clear we're on cusp of AGI. Government decides on Project. Security crash program must be complete.
- **2027**: Projected AGI arrival. Systems capable of automating cognitive work as drop-in remote workers.
- **2027-2028**: Intelligence explosion period. Automated AI research compresses decade of progress into year or less.
- **2028-2030**: Superintelligence achieved. Most volatile period in human history begins.
- **2030s**: Economic explosion, military transformation, new world order forged—if we make it through.

## Critical Failure Points

1. **Algorithmic Security (12-24 month window)**: Failure to protect algorithmic secrets means China matches US despite compute disadvantage.

2. **Weight Security (3-4 year window)**: Need state-proof security before AGI. Otherwise deliver superintelligence to CCP, North Korea, terrorists.

3. **Superalignment (narrow window during intelligence explosion)**: Rapidly going from human-level to vastly superhuman with little time to ensure we can trust systems before stakes become catastrophic.

4. **International Race Dynamics**: Neck-and-neck race means no margin for safety. Need healthy lead (1-2 years not 1-2 months) to navigate challenges.

5. **Proliferation**: Without nonproliferation regime, rogue states and terrorists get super-WMDs.

## Author's Key Arguments

**On Timelines**: "AGI by 2027 requires no esoteric beliefs, merely trend extrapolation of straight lines." The math is simple: another 100,000x effective compute from GPT-4 to 2027 produces another preschooler-to-high-schooler jump, on top of already-expert systems.

**On Security**: "Our failure today will be irreversible soon: in the next 12-24 months, we will leak key AGI breakthroughs to the CCP. It will be the national security establishment's single greatest regret before the decade is out."

**On Safety**: "We're counting way too much on luck here." Not on track for managing intelligence explosion, no sane chain of command, labs haven't shown willingness to make costly safety tradeoffs.

**On The Project**: "I find it an insane proposition that the US government will let a random SF startup develop superintelligence. Imagine if we had developed atomic bombs by letting Uber just improvise."

**On Stakes**: "Will the free world prevail? Will we tame superintelligence, or will it tame us? Will humanity skirt self-destruction once more? The stakes are no less."

## Historical Parallels

Throughout, Aschenbrenner draws parallels to nuclear weapons development:

- **Szilard and Einstein's warnings** (1939-1940) initially dismissed, eventually heeded just in time
- **German wrong turn** on graphite due to Fermi's results being kept secret—decisively handicapping their program
- **Manhattan Project mobilization** - delayed, hamfisted, but ultimately extraordinary government response
- **Atoms for Peace / NPT** - model for benefit-sharing and nonproliferation regime
- **The Bomb vs. The Super** - AGI vs. Superintelligence as atomic vs. hydrogen bomb-level capability jump

## About This Wiki

Source material: Leopold Aschenbrenner's "Situational Awareness: The Decade Ahead" essay series, published June 2024. Author previously worked at OpenAI on superalignment research with Ilya Sutskever's team.

These essays represent perspective of those "in the room"—among the "few hundred people" with situational awareness who understand what's being built. Whether right or wrong about specific claims, this perspective from leading AI researchers is historically significant.

The wiki decomposes content by concept rather than source structure, aiming for information density and utility for answering questions about the decade ahead.
