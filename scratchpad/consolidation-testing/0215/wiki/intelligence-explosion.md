# Intelligence Explosion

An intelligence explosion occurs when AI systems become capable of improving AI research itself, creating a rapid feedback loop that compresses a decade of progress into a year or less.

## The Basic Mechanism

Once AGI can automate AI research, progress accelerates dramatically:
1. Create AI systems capable of doing AI researcher/engineer work
2. Run millions of copies of these automated researchers (100M+ human-equivalents)
3. These researchers work 24/7 at 10-100x human speed
4. They achieve in 1 year what would take humans a decade
5. Result: 5+ [[orders-of-magnitude-scaling]] of algorithmic progress in under a year

This represents another GPT-2-to-GPT-4-sized qualitative jump—from preschooler to smart high schooler—but on top of already-expert AI systems.

## Timeline

Plausibly <1 year from fully automated AI researchers to vastly superhuman systems. Conservative estimate: compress human-decade of algorithmic progress (~0.5 OOMs/year × 10 years = 5 OOMs) into ≤1 year.

## The Scale of Automated Research

By 2027, GPU fleets should support:
- ~10s of millions of A100-equivalents in training clusters
- Much larger inference fleets
- Ability to run ~100 million human-researcher-equivalents simultaneously
- Each researcher working day and night
- Generation of an entire internet's worth of tokens daily

Calculation: With 10M+ A100-equivalent GPUs, API-cost-equivalent token generation, assuming humans "think" at 100 tokens/minute, yields ~200M human-equivalents running continuously.

## Advantages Over Human Researchers

Automated AI researchers will have extraordinary advantages:
- Read every ML paper ever written
- Internalize every experiment ever run at the lab
- Learn in parallel from all copies
- Accumulate millennia-equivalent of experience in weeks
- Write millions of lines of code, maintaining entire codebase in context
- Perfect memory and tireless focus
- No individual training needed—teach one, copy to millions
- Share context and latent-space-level understanding
- Improve continuously as better models emerge

## Speed Multipliers

Beyond just quantity, automated researchers will be fast:
- Initial systems: ~5x human speed
- With inference tradeoffs: ~100x human speed (trading parallel copies for serial speed)
- Early focus: Find 10-100x algorithmic speedups (Gemini 1.5 Flash is ~10x faster than GPT-4 at similar capability)
- Result: 100M researchers × 100x speed = effectively 10 billion human-researcher-years/year

## Plausibility

Major ML breakthroughs of the last decade were often straightforward hacks: LayerNorm/BatchNorm ("add normalization"), residual connections ("do f(x)+x instead of f(x)"), Chinchilla scaling laws ("fix implementation bug"). AI research is automatable. The job of AI researcher/engineer at frontier labs can be done fully virtually—read literature, implement experiments, interpret results, repeat.

Moreover, automating AI research doesn't require automating everything (like robotics or biology R&D), avoiding bottlenecks those domains face.

See also: [[superintelligence-definition]], [[automated-ai-research]], [[recursive-self-improvement]]
