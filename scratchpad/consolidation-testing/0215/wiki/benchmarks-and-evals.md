# Benchmarks and Evals

Benchmarks and evaluations are the empirical tests demonstrating AI capability progress, though AI is rapidly saturating even the hardest ones.

## The Saturation Problem

We're literally running out of benchmarks. AI systems are rapidly reaching or exceeding human-level in many domains. What used to take decades to crack now takes mere months.

**MMLU (Massive Multitask Language Understanding)**:
- Created in 2020 by Dan Hendricks and Collin Burns
- Hoped to finally make benchmark standing test of time
- Equivalent to all hardest exams given to high school and college students
- Just 3 years later: basically solved
- GPT-4 and Gemini get ~90%

## Standard Test Saturation

GPT-4 mostly cracks all standard high school and college aptitude tests. Often, one year from GPT-3.5 to GPT-4 took us from well below median human performance to top of human range.

**Examples from GPT-4**:
- AP exams: Top percentiles
- SAT: Top percentiles
- GRE: Top percentiles
- Bar exam: Top percentiles
- Medical licensing: Top percentiles
- Various olympiad competitions: Strong performance

And no, these tests aren't in training set. AI labs put real effort into ensuring evals are uncontaminated because they need good measurements for science.

## MATH Benchmark Case Study

Released 2021: Difficult mathematics problems from high-school math competitions.

**Original expectation**: "Moreover, we find that simply increasing budgets and model parameter counts will be impractable for achieving strong mathematical reasoning [...]. To have more traction [...] we will likely need new algorithmic advancements."

**Forecaster predictions**: ML researchers predicted minimal progress over coming years.

**Reality**:
- Initial performance: ~5%
- Mid-2022 (1 year later): ~50% (Minerva)
- 2024: >90% (basically solved)

This pattern repeats: Skeptics say "deep learning won't be able to do X," then proven wrong within months.

## Current Frontier: GPQA

GPQA: PhD-level biology, chemistry, and physics questions. Many questions read like gibberish to laypeople. PhDs in other scientific fields spending 30+ minutes with Google barely score above random chance.

**Current performance**:
- Claude 3 Opus: ~60%
- Domain expert PhDs: ~80%

Expected to fall in next generation or two. We're already better than general humans at PhD-level science questions. Soon we'll crack expert-PhD-level.

## The Progression

**Decade ago**: Deep learning revolutionary for identifying simple images

**GPT-2 (2019)**: Barely string together semi-coherent sentences, can't count to 5 reliably

**GPT-3 (2020)**: Elementary school level, basic poetry and simple coding

**GPT-4 (2023)**: Smart high schooler, acing college exams, sophisticated coding with debugging

**Next frontier**: PhD-level expertise across all domains

## Benchmark Creation Challenges

Making hard-enough benchmarks is difficult. As soon as one is created and seems challenging, it's quickly saturated. This is mostly a reflection on difficulty of making hard-enough benchmarks, not saying much about ultimate capabilities.

Any benchmark we have today will be saturated soon. The challenge is creating tests that measure truly superhuman capabilities.

## Historical Underestimation

**Professional forecasters consistently wrong**: Gray forecasts made in August 2021 for June 2022 MATH performance put actual performance far exceeding even upper range. Median ML researcher even more pessimistic.

**Bryan Caplan betting record**: Professor with perfect public betting record bet in January 2023 that no AI would get A on his economics midterm by 2029. Two months later, GPT-4 scored an A (one of highest scores in his class).

**Gary Marcus walls**: Predicted walls after GPT-2 solved by GPT-3. Walls predicted after GPT-3 solved by GPT-4.

## Why It Matters

Benchmarks provide empirical grounding for [[scaling-laws]] and capability predictions. They're how we know the trendlines are real, not just theoretical extrapolation. Each time skeptics claim "this time it's different," benchmarks prove them wrong.

The consistent pattern: If you keep being surprised by AI capabilities, just start counting the OOMs.

See also: [[scaling-laws]], [[agi-definition-and-timeline]], [[gpt-progression]]
