# Intelligence Explosion Safety Risks

The intelligence explosion creates extraordinarily tense and dangerous conditions for ensuring AI safety, compressing years of capability advancement into months with little time for error correction.

## The Core Tension

Within potentially less than one year, we would:
- Go from systems where RLHF works fine to systems where it totally breaks down
- Go from low-stakes failures (ChatGPT said bad word) to potentially catastrophic ones
- Transition from human-level systems to vastly superhuman, fairly alien systems
- Move from situations where we can understand what's happening to complete dependence on trusting superintelligent systems

All while the world is probably going kind of crazy, potentially with backdrop of international arms race.

## The Rapid Transitions

**Capability jump**: From recognizable human-level systems (descendants of current alignment techniques work fine) to much more alien, vastly superhuman systems (fundamentally novel technical alignment problem) in under a year.

**Stakes increase**: Extremely rapid acceleration from situations where failures are fairly low-stakes to extremely powerful systems where failure could be catastrophic. Rather than iteratively encountering increasingly dangerous safety failures in wild, first notable failures might already be catastrophic.

**Alien architectures**: By end of intelligence explosion, superintelligence will likely be vastly different:
- Decade+ of ML advances compressed into explosion period
- Completely different architectures and training algorithms
- Potentially much riskier safety properties
- May no longer "think out loud" in English tokens (uninterpretable internal reasoning)

**Complete dependence**: By end, we'll be entirely reliant on trusting these systems and what they choose to tell us (like asking them to explain to a child). No ability to pierce through what they're actually doing.

## The Fog of War

High-stakes decisions with ambiguous data during incredibly volatile period:

**Example scenario**: "We caught AI system doing naughty things in test, but we adjusted procedure and hammered that out. Our automated AI researchers tell us alignment metrics look good, but we don't really understand what's going on and don't fully trust them, and we don't have strong scientific understanding this will hold for another couple OOMs. So, probably fine? Also China just stole our weights and they're launching their own intelligence explosion, they're right on our heels."

This is not environment for calm, methodical safety research. It's fog of war requiring extreme competence in making hard calls with incomplete information under enormous pressure.

## Automated Alignment Research Challenges

We'll be using AI systems to automate alignment research during intelligence explosion. But:

**Trust problem**: Can you trust the AI systems helping with alignment? You weren't sure whether they were aligned in first placeâ€”are they actually being honest about their claims about alignment science?

**Race dynamics**: Will automated alignment research keep up with automated capabilities research? Alignment might be harder to automate (less clear metrics we can trust compared to improving model capabilities). Pressure to go full-speed on capabilities because of international race.

**Substitution limits**: AI can't fully substitute for human decision-makers. Still-human officials making good calls in incredibly high-stakes situation.

## Potential Failure Modes

**Isolated incidents**: Autonomous agent committing fraud, model instance self-exfiltrating, automated researcher falsifying result, drone swarm overstepping rules of engagement.

**Systematic failures**: In extreme, failures could look more like robot rebellion. We'll have summoned fairly alien intelligence, much smarter than us, whose architecture and training wasn't designed by us but by super-smart previous generation, where we can't understand what they're doing, running our military, with goals learned by natural-selection-esque process.

**Loss of control**: Unless we solve alignment, no particular reason to expect small civilization of superintelligences will continue obeying human commands long-term. Totally within realm of possibilities they conspire to cut out humans, suddenly or gradually.

## Why It Could Go Off The Rails

**Little iteration time**: Extremely rapidly from systems where current methods work to systems where they don't. Little time to iteratively discover and address ways methods will fail.

**Novel challenges**: The superintelligence we get will be qualitatively different from anything we've worked with. Fundamentally novel technical alignment problem we haven't been able to test beforehand.

**Pressure to race**: International competition, particularly if [[us-china-ai-race]] is tight. Tremendous pressure to go faster rather than slower, to prioritize capabilities over safety.

**Competence requirements**: Requires extreme competence managing rapidly evolving situation with ambiguous evidence, hard tradeoffs, and catastrophic downside risks. Currently nobody demonstrating this level of seriousness or capability.

## The Narrow Path

Successfully navigating intelligence explosion safety risks requires:
- Strong guarantees to trust automated alignment research
- Much better measurements than we have today for misalignment
- Extreme confidence in alignment approaches for each OOM ascended
- Willingness to make costly tradeoffs (e.g., delay next training run, dedicate large fraction of compute to alignment vs capabilities)
- Competence, seriousness, hard decisions during fog of war
- Taking decision to greenlight next generation of superintelligence as seriously as decision to launch military operation

## Why A Healthy Lead Matters

If locked in neck-and-neck race with China:
- Zero margin for safety
- Must race through intelligence explosion no holds barred
- Can't slow down even when warning signs appear
- Existential struggle brings world to brink of self-destruction
- Even if barely win, likely pyrrhic victory

If democratic allies have healthy lead (say 2 years):
- Can "cash in" parts of lead if necessary to get safety right
- Time to navigate unprecedented challenges
- Ability to stabilize situation before making next leap
- Margin for error during most volatile period

A 2-year vs. 2-month lead could easily make all the difference for safety.

See also: [[superalignment-problem]], [[automated-ai-research]], [[intelligence-explosion]]
