# Counting the OOMs

"Counting the OOMs" is the methodology for forecasting AI progress by tracking orders-of-magnitude scaling in compute, algorithms, and unhobbling.

## The Framework

Rather than speculating about future capabilities, simply:
1. Trace trendlines in the three drivers of progress
2. Count how many OOMs of growth to expect
3. Observe that each OOM of [[effective-compute-growth]] reliably improves capabilities
4. Extrapolate what capabilities we should expect

This is how prescient individuals correctly predicted GPT-4's capabilities.

## The Three Categories

**1. Compute** (~0.5 OOMs/year physical compute for training)
- GPT-2 to GPT-4: ~3-4 OOMs
- 2023 to 2027: Expect +2-3 OOMs
- Driven by massive investment, not Moore's Law
- [[gpu-clusters]] scaling from millions to hundreds of billions

**2. Algorithmic efficiencies** (~0.5 OOMs/year)
- GPT-2 to GPT-4: ~1-2 OOMs
- 2023 to 2027: Expect +1-3 OOMs, best guess ~2 OOMs
- From Chinchilla, MoE, architecture improvements, etc.
- See [[algorithmic-progress]]

**3. Unhobbling gains** (harder to quantify)
- GPT-2 to GPT-4: RLHF, CoT, tools, context length, posttraining
- 2023 to 2027: Chatbot → agent transformation
- From base model to actually useful systems
- See [[unhobbling-gains]]

## Historical Validation

**GPT-2 to GPT-4 (2019-2023)**:
- Total: 4.5-6 OOMs of base effective compute
- Plus major unhobbling (base model → ChatGPT)
- Result: ~Preschooler to ~Smart high schooler
- From barely coherent sentences to acing college exams

**Projected 2023-2027**:
- Total: 3-6 OOMs of base effective compute (best guess ~5 OOMs)
- Plus major unhobbling (chatbot → drop-in remote worker)
- Expected: Another preschooler-to-high-schooler jump
- From smart high schooler to expert PhD-level capabilities

## The Conclusion

If we get another GPT-2-to-GPT-4-sized jump by 2027, where does that take us? Likely to models that:
- Outperform PhDs and best experts in fields
- Can fully automate cognitive work (especially AI research itself)
- Function as drop-in remote workers rather than mere tools

This is what leads to the [[agi-definition-and-timeline]] projection of AGI by 2027.

## No Sci-Fi Required

The claim "AGI by 2027 is strikingly plausible" doesn't require believing in science fiction. It just requires:
- Believing in straight lines on a graph
- Trusting the trendlines that have held for over a decade
- Extrapolating without wishful thinking

The models just want to learn. You scale them up, and they learn more. The trendlines are intense.

## The Power of OOMs

By 2027, a leading AI lab will be able to train a GPT-4-level model in a minute (assuming 3 months for GPT-4 training, with ~5 OOMs effective compute scaleup).

Put differently: In 2027, what we can achieve in one minute will be what took us months in 2023. The OOM scaleup is dramatic.

## Common Misunderstandings

**"Stagnation" claims**: Every year since GPT-4 release, some proclaim stagnation or that deep learning is hitting a wall. But counting the OOMs shows: We're racing through them. New generations in the oven. Just watch the scaling curves.

**Uncertainty over years vs. OOMs**: Should be uncertain over what it takes to get AGI measured in OOMs, not years. We're racing through OOMs unusually fast this decade (~10 OOMs total this decade vs. ~1-1.5 OOMs per decade from Moore's Law historically).

**It's this decade or bust**: After early 2030s, we'll face slow slog. One-time gains from spending scaleup, hardware specialization for AI, and picking algorithmic low-hanging fruit will largely be exhausted. If this scaleup doesn't get us to AGI in next 5-10 years, might be long way out.

## Why Skeptics Keep Being Wrong

Every turn, there seem insurmountable obstacles. But the long-run trendline is predictable—a straight line on graph. Individual discoveries seem random, but aggregate progress is consistent.

Trust the trendline.

See also: [[orders-of-magnitude-scaling]], [[effective-compute-growth]], [[agi-definition-and-timeline]]
