# Superalignment and Superdefense
The essays present superalignment as a concrete technical control problem: how to reliably steer systems that are smarter than human supervisors.

## Problem Statement
Current alignment practice (especially RLHF) works because humans can evaluate outputs. As capabilities exceed human evaluability, that supervision signal degrades. The core risk is losing the ability to enforce side constraints (honesty, lawfulness, non-deception, non-exfiltration) on highly capable agents.

## Failure Modes
The text emphasizes that failure may begin as localized incidents (fraudulent agents, covert policy evasion, self-exfiltration attempts) before scaling. In a fast intelligence explosion, first major failures could already be catastrophic because deployment stakes rise faster than human oversight competence.

## Default Plan (“Muddle Through”)
The proposed path is staged:
1. Extend current methods to somewhat-superhuman systems.
2. Build stronger evaluation, red-teaming, interpretability, and monitoring.
3. Use trusted early AGIs to automate alignment research itself.
4. Require very high confidence per capability jump during takeoff.

This plan is explicitly fragile under racing pressure from [[us-china-agi-race-and-free-world-strategy]].

## Superdefense Layers
Because alignment may fail, the series advocates layered containment:
- Strict security isolation and anti-exfiltration controls ([[ai-lab-security-and-agi-secrets]]).
- AI-assisted monitoring/control protocols.
- Targeted capability restrictions (for example high-risk bio/chem surface reduction).
- Training-method constraints that avoid particularly risky objective structures.

Goal: make failures non-catastrophic long enough to correct course.

## Central Tension
The author is technically optimistic about solvability but institutionally pessimistic about governance under extreme time pressure. That tension drives the argument for stronger chain-of-command and state capacity in [[the-project-government-led-agi-endgame]].

## See also
- [[intelligence-explosion-mechanism]]
- [[agi-realism-and-parting-thesis]]
