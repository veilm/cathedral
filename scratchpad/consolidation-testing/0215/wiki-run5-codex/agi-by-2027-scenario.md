# AGI by 2027 Scenario
The series’ headline forecast is that AGI by 2027 is “strikingly plausible” under extrapolation of current compute, algorithmic, and unhobbling trends.

## Operational Definition of AGI
The essay rejects a weak “good chatbot” definition. AGI is defined functionally as systems that can perform the job of human AI researchers/engineers and broadly automate remote cognitive labor.

That includes sustained task execution, reasoning, coding, tool use, and adaptation to organizational context.

## Quantitative Backbone
The scenario combines:
- Continued compute trend near ~0.5 OOM/year.
- Comparable order algorithmic-efficiency trend.
- Additional step-change gains from unhobbling.

The rough synthesis is another GPT-2→GPT-4-sized capability jump by end-2027. One illustrative claim: if GPT-4 required ~3 months to train, a leading lab in 2027 could train a GPT-4-equivalent in roughly a minute (under best-guess effective-compute assumptions).

## Why 2020s Are Privileged
A key meta-argument is that uncertainty should be expressed in OOM requirements, not years. Because the 2020s feature unusually rapid OOM traversal (capital concentration, hardware specialization, aggressive industrial buildout), probability mass should concentrate this decade rather than being evenly spread over many decades.

## Main Sources of Uncertainty
The scenario can fail or delay if:
- Data-wall solutions are harder than expected ([[data-wall-and-post-llm-paradigms]]).
- Agentic unhobbling plateaus ([[unhobbling-and-agent-coworkers]]).
- Infrastructure/power/security bottlenecks bind hard ([[trillion-dollar-cluster-buildout]], [[ai-lab-security-and-agi-secrets]]).

Still, the thesis is that policymakers should plan as if this path is realistic, because downside of under-preparation is extreme.

## See also
- [[counting-the-ooms-framework]]
- [[intelligence-explosion-mechanism]]
