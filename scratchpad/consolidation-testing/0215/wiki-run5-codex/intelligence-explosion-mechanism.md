# Intelligence Explosion Mechanism
The second-stage thesis is that once AI can automate AI research, progress can accelerate from rapid to explosive.

## Core Loop
1. Build AGI systems capable of meaningful AI R&D.
2. Run many copies on massive inference fleets.
3. Use them to improve algorithms, inference speed, and training methods.
4. Reinvest gains into stronger systems and larger effective research labor.

This is framed as a practical recursive-improvement loop, not speculative magic.

## Magnitude Claims
The essays present order-of-magnitude estimates like:
- Inference capacity supporting ~100 million human-researcher-equivalents (ballpark), with potential speedups to 10xâ€“100x human pace.
- Compression of ~10 human-years of algorithmic progress into <=1 year.
- 5+ OOM algorithmic gains in a year as a conservative explosive scenario.

Even partial realization yields a very fast transition from AGI to much more superhuman systems.

## Bottlenecks Considered
The series discusses several brakes:
- Compute limits for running experiments.
- Complementarity with remaining human bottlenecks.
- Diminishing returns/idea difficulty.
- Hard ceilings on algorithmic efficiency.

The conclusion is these likely soften, not negate, acceleration; compute-for-experiments is treated as the most important near-term bottleneck.

## Broader Expansion
Initial explosion is narrow (AI R&D). Then, with superhuman systems, acceleration broadens into:
- Robotics progress.
- General scientific/engineering advance.
- Industrial buildout and potentially new growth regime.
- Military capability leaps with destabilizing geopolitical effects.

This connects directly to [[superintelligence-power-and-economic-regime-shift]] and [[us-china-agi-race-and-free-world-strategy]].

## See also
- [[agi-by-2027-scenario]]
- [[superalignment-and-superdefense]]
