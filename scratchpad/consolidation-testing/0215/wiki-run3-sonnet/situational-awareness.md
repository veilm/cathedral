# Situational Awareness

Situational awareness refers to understanding what is actually happening with AI progress and its implications—seeing the trajectory toward AGI and superintelligence clearly.

## The Few Hundred

Right now, there are perhaps a few hundred people in the world who:
- Realize what's about to hit us
- Understand just how crazy things are about to get
- Have internalized the implications of the trendlines
- See how AGI will be built (the cluster, the algorithms, the people, the timeline)

Most of these people are in San Francisco and the AI labs. They were derided as crazy a few years ago—but they trusted the trendlines, which allowed them to correctly predict the AI advances of the past few years.

## The Mainstream Gap

**What mainstream sees:**
- Hype and business-as-usual
- At most, another internet-scale technological change
- Nvidia analysts thinking 2024 might be close to peak
- Pundits stuck on "it's just predicting the next word"
- "Deep learning is hitting a wall" (proclaimed every year for past decade)

**What those with situational awareness see:**
- AGI by 2027 as strikingly plausible
- Intelligence explosion within year after AGI
- Superintelligence by 2030
- Most powerful weapon mankind has ever built
- Decisive military advantage at stake
- Need for unprecedented industrial mobilization
- Existential challenge for national security

## How To Get It

"If you keep being surprised by AI capabilities, just start counting the OOMs."

Situational awareness isn't actually that hard once you:
1. Step back and look at the trends ([[orders-of-magnitude-scaling]])
2. Take the trendlines seriously (they've been astonishingly consistent)
3. Extrapolate without wishful thinking or denial
4. Consider what the implications actually are if we're right

The trendlines look innocent, but their implications are intense.

## The Investment Edge

Those with situational awareness bought Nvidia "much lower than you, but it's still not even close to fully priced in." The magnitude of what's coming—$1T clusters, $10T companies, superintelligence, economic explosion—isn't remotely priced into markets yet.

## The Burden

"I had many sleepless nights. But I did realize how very very serious it could be. And I had then to start taking sleeping pills. It's the only remedy, I've never stopped since then. It's 28 years, and I don't think I've missed a single night in all those 28 years." —James Chadwick, 1941, on realizing atomic bomb was inevitable

For those with situational awareness now:
- It's starting to feel very real, very visceral
- Can see exactly how it will happen
- Realize there's no crack team coming to handle this
- Understand the burdens of history are heavy
- Bear responsibility for what happens next

## The Responsibility

The fate of the world rests on the few hundred people with situational awareness and their close networks. That's all there is. These are great and honorable people. But they are just people.

Someday it will be out of our hands. But right now, for the next few years of midgame, they will determine:
- Will the free world prevail?
- Will we tame superintelligence, or will it tame us?
- Will humanity skirt self-destruction once more?

The stakes are no less.

## AGI Realism

The third way between doomers and e/accs:

1. **Superintelligence is matter of national security**: We are rapidly building machines smarter than smartest humans. Not another Silicon Valley boom—most powerful weapon mankind has ever built.

2. **America must lead**: Can't "pause." Must scale US power production, build AGI clusters in US, lock down security. AI labs must put national interest first.

3. **We need to not screw it up**: Real safety risks exist. Navigating perils will require seriousness and competence not yet offered. Improvising won't cut it.

See also: [[agi-definition-and-timeline]], [[counting-the-ooms]], [[intelligence-explosion]]
