# Model Weights Security

Model weights are the trained parameters of an AI system—literally just a large file of numbers that, if stolen, give adversaries a complete copy of potentially superintelligent AI.

## The Threat

An AI model is just a file on a server. All it takes for an adversary to match trillions of dollars, decades of work, and America's smartest minds is to steal this file. Imagine if the Nazis had gotten exact duplicates of every atomic bomb made in Los Alamos.

Current AI lab security: essentially zero against state actors. Labs admit to "level 0" security when rated on frameworks requiring "level 3" (defense against North Korea) or "level 4" (defense against China's MSS).

## Why It Matters

**National competition**: If China steals AGI weights on cusp of [[intelligence-explosion]], they can immediately launch their own intelligence explosion, even if previously far behind. Any US lead vanishes instantly.

**Safety margin**: A stolen-weights scenario forces existential race through intelligence explosion. Both sides race at maximum speed, with China potentially willing to skip safety precautions. No margin for getting alignment right. A 1-2 year lead becomes 1-2 months, transforming manageable safety challenges into catastrophic pressure.

**Proliferation**: On current course, superintelligence weights would be accessible to North Korea, Iran, rogue states, and potentially terrorist groups. Novel WMDs superintelligence could invent would proliferate to dozens of dangerous actors.

## The Timeline Problem

Weights security doesn't matter much today (GPT-4 theft isn't catastrophic). But it will matter critically in a few years with AGI/superintelligence. The problem: developing infrastructure for weight security takes many years of lead time.

**If we think AGI in ~3-4 years is possible, we need to launch the crash effort now.**

Failure to prepare in time creates dire choice: Press ahead but directly deliver superintelligence to CCP, or wait years for security while risking losing lead.

## What's Required

State-actor-proof security requires government involvement and includes:

**Physical security:**
- Fully airgapped datacenters with military-base-level fortifications
- Cleared personnel only
- Physical fortifications, onsite response teams
- Extensive surveillance, extreme access control
- Applies to both training AND inference clusters (inference fleets will be larger and also need protection)

**Technical measures:**
- Novel hardware encryption / confidential compute
- Extreme scrutiny of entire hardware supply chain
- Multi-key signoff to run any code
- Defense-in-depth (can't rely only on hardware encryption—it gets side-channeled)

**Personnel security:**
- Extreme vetting and security clearances
- Working from SCIFs (Sensitive Compartmented Information Facilities)
- Regular integrity testing
- Constant monitoring
- Substantially reduced freedom to leave
- Rigid information siloing

**Network security:**
- Strict limitations on external dependencies
- TS/SCI network requirements
- Ongoing intense penetration testing by NSA

## Development Timeline

Securing weights requires innovations in hardware and radically different cluster design. Security at this level can't be reached overnight—it requires cycles of iteration. The security crash program must start now, years before AGI, or we won't be ready.

## The Cost of Failure

The worst-case scenario: China steals [[automated-ai-research]] model weights on cusp of intelligence explosion. This would allow China to immediately automate AI research themselves and build [[superintelligence-definition]], eliminating any US lead and forcing a breakneck race with zero safety margin.

See also: [[algorithmic-secrets-security]], [[ai-lab-security]], [[chinese-espionage]]
