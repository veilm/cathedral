# Designing a Long-Term Memory Scaffolding for LLMs

## Introduction

Large language models (LLMs) excel at processing recent context but struggle with long-term retention. Without a specialized memory system, an LLM quickly “forgets” details from earlier conversations once they slip out of the context window. For example, a user might spend weeks chatting about their favorite foods or personal goals, only to find the LLM later repeats questions or suggestions it should already know are irrelevant. This is because standard LLMs rely almost entirely on short-term memory (the immediate prompt) and have no built-in mechanism to **store and recall past interactions** over long periods.

In contrast, humans possess a rich long-term memory. A close friend remembers your habits, hobbies, and important life events, even if you last spoke weeks ago. They recall that you're allergic to peanuts or that you already dismissed a book recommendation last month. They maintain a continuous understanding of **who you are**, built up from repeated interactions. The goal of this proposal is to design an **external long-term memory scaffolding** for LLMs – a model-agnostic system that gives an LLM the ability to retain and recall past information like a human companion would. This memory system will be maintained locally (all data stored on the user’s device) while using the LLM’s API only for processing, ensuring privacy and compliance with cost constraints. The design emphasizes efficiency (minimal extra latency or cost per query) and real-time recall (the LLM should answer using memories within a few seconds to a couple minutes at most). It should continuously update whenever new information emerges, so the LLM “learns” from each conversation in a persistent way.

Before presenting the design, we review how human memory works and survey existing attempts to give LLMs long-term memory. These insights will inform a **hybrid memory architecture** that balances strengths and weaknesses of various approaches.

## Human Memory: Inspiration for Long-Term AI Memory

Human memory is often viewed in a hierarchy of **short-term** and **long-term** stores. In the brain, **working memory** (short-term) holds a small amount of information briefly (like a phone number you just heard) whereas **long-term memory** can store vast amounts of information for extended periods (from minutes to a lifetime). Long-term memory itself has distinct types: **episodic memory** (recollections of experiences and events in context, like remembering a past conversation or a vacation) and **semantic memory** (general knowledge and facts, like knowing Paris is the capital of France, or that your friend likes classical music). Humans also have implicit memories (skills and habits), but in an LLM context we focus on the explicit memories (analogous to episodic and semantic data in conversations).

Several characteristics of human memory are relevant:

* **Encoding and Consolidation:** Not everything we encounter is stored with equal strength. The brain tends to encode information that has meaning or emotional importance more deeply. Repetition greatly increases the likelihood of moving information into long-term memory. Over time, the brain **organizes** memories, associating related information (e.g. linking an object with its context). For our LLM scaffolding, this suggests we should preferentially preserve important or frequently referenced information. Repeated facts (like a user mentioning their cat’s name multiple times) should be strongly stored, and related pieces of info should be linked or grouped for easier retrieval.

* **Retrieval by Cues:** Human memory retrieval is cue-dependent. We often need a prompt or context similar to the original situation to recall something – for example, visiting a location can trigger memories of events that happened there. Effective retrieval involves matching the right cues to stored information. In our design, this translates to using the current conversation context or user query as a cue to find relevant stored memories. Techniques akin to **semantic search** (finding memories with similar meaning to the query) mimic this process by providing the LLM with memory snippets closely related to the conversation at hand.

* **Forgetting and Relevance:** Human long-term memory isn’t an exact log of everything; it’s dynamic. We naturally **forget** details that we don’t revisit or that become irrelevant, a phenomenon quantified by the *Ebbinghaus forgetting curve* (rapid initial forgetting of unused information). Some theories suggest forgetting often results from interference – as more memories accumulate, similar ones can interfere and make specific recall harder. There’s also evidence of **active forgetting**: the brain appears to deliberately prune or suppress memories that are redundant or not needed, to declutter and make important memories more accessible. On the other hand, **frequently recalled** memories get reinforced – every time you recall something, you essentially “reset” its decay and strengthen that memory (akin to spaced repetition). This guides our system to implement *selective forgetting* (aging out or compressing seldom-used data) and *memory reinforcement* (keeping track of which facts or episodes are recalled often and prioritizing them). By mirroring these human strategies, the LLM’s memory can remain both relevant and manageable over time.

* **Abstraction and Gist:** Humans tend to remember the **gist** of conversations or events rather than every word. We condense a day’s experiences into high-level summaries (e.g. “I had a fun chat about music with Alice”) and only recall details when needed. This hierarchical summarization – remembering summaries of experiences, with some key details attached – is a natural compression strategy. It lets us store long periods of experiences in a relatively compact form. Our LLM memory design will likewise use **summarization** to compress long interactions into concise notes, capturing important points without storing entire transcripts. By storing multi-level summaries (summaries of summaries), the system can provide high-level context and drill down to finer details on demand. This mimics how a person might first recall “We talked about planning a trip last week” and then, if needed, recall specific details like the destination or dates discussed.

**Implications for LLM Memory:** The LLM’s external memory should distinguish between short-term (working) memory and long-term memory, and within long-term memory it should manage both episodic event memory and semantic fact memory. It should reinforce important information through repetition, use contextual cues for retrieval, allow less useful info to fade, and employ summarization to keep a high-level understanding of past dialogues. In short, the system should **store everything important**, but also **organize and prune** it so that the most relevant pieces are readily available – achieving an efficient, human-like balance of retention and forgetting.

## Existing Approaches to Long-Term Memory in LLMs

Developers and researchers have tried multiple strategies to give LLMs a longer memory. We examine the major approaches, highlighting their principles, advantages, and limitations:

### 1. **Summarization-Based Memory**

One straightforward approach is to **summarize past interactions** and provide those summaries to the LLM instead of raw transcripts. As a conversation grows, older messages are distilled into a shorter synopsis that captures key facts or decisions. This summary is kept in the prompt (or stored externally and prepended on each query) to give context. Some systems perform *recursive or hierarchical summarization*: summarizing interactions periodically (e.g. every N messages or every session), and if those summaries themselves become numerous, summarizing the summaries, and so on. This results in a pyramid of memory: high-level summaries (e.g. by topic or time period) that can be expanded into more detailed sub-summaries, down to raw logs if needed.

This method **mimics the human tendency to recall the gist** and allows theoretically unbounded conversations by keeping context length fixed. For example, Anthropic’s research and applications like Pieces.app use hierarchical summarization to condense months of activity into manageable chunks. A top-level summary helps narrow down relevant parts, then detailed summaries of those parts provide concrete context to feed the LLM. One benefit is that it’s adaptable to different LLM context sizes: if using a model with a small context window, you stick to high-level summaries; a larger context model can afford more detailed memory.

However, summarization has important downsides. **Loss of detail** is inevitable – nuance or specific facts might be dropped if the summary is too high-level. There's also risk of **summarization error or drift**: the model generating the summary might introduce inaccuracies or *hallucinations*, especially if done repeatedly. These errors can propagate; a mistake in an early summary will likely appear in later summaries-of-summaries, compounding the problem. If the LLM’s answers rely on a flawed summary, it could confidently assert incorrect “memories.” In practice, careful techniques are needed to mitigate this, like verifying summaries against source logs or limiting how much abstraction is done in one step. Additionally, summarizing after each user turn can be expensive (each summary is an extra LLM call), so systems might summarize less frequently (say, at conversation breaks or when context limits hit).

**Assessment:** Summarization is **promising for compressing information** and keeping the context window free, and it's fairly model-agnostic – any LLM can read a summary. But it should be paired with strategies to preserve critical details (perhaps storing those details separately) and to avoid cascading errors. In an LLM memory scaffold, summaries are best used to complement raw memory rather than replace it entirely – for example, maintain full detailed logs in storage but use summaries for quick context and navigation.

### 2. **Vector Database (Embedding-Based) Memory**

Another prevalent strategy is **semantic vector memory**. Instead of trying to cram all past info into the prompt, we store past conversation chunks or facts in a database as vector embeddings, and retrieve relevant pieces **on demand**. This is essentially an application of Retrieval Augmented Generation (RAG) for personal memory: the conversation history becomes an external knowledge base that the LLM can query.

In practice, each piece of information (a message, a paragraph of notes, a fact extracted from conversation) is converted into an embedding – a numerical vector representing its semantic content. These embeddings are stored in a local vector store (e.g. using libraries like FAISS or a vector database like Pinecone, depending on local vs cloud) for fast similarity search. When a new user query comes in, the system generates an embedding for the query or the recent dialogue context, and finds the stored memory vectors closest in meaning. The retrieved top-\$k\$ memory items (e.g. the 3–5 most relevant snippets) are then inserted into the LLM’s prompt as additional context. This way, the LLM’s answer can be grounded not only in its built-in knowledge, but also in specific details from the user’s history that it otherwise wouldn’t remember.

This approach has been implemented in various tools. For example, LangChain’s **VectorStoreRetrieverMemory** and similar utilities save conversation transcripts or notes as embeddings and pull in relevant ones for each new question. AutoGPT and other autonomous agents have used vector databases (like Pinecone) to store long-term facts or goals, though early versions struggled to actually utilize that memory effectively. The **MemoryBank** system by Zhong et al. uses a dense retrieval mechanism (FAISS) to efficiently search a repository of past dialogues and summarized events, enabling the AI to recall, say, a past recommendation or a personal detail when it becomes relevant in conversation.

**Advantages:** Embedding-based memory means **nothing is truly forgotten** (unless purged) – the raw info is out-of-context but always accessible via search. It’s also **fine-grained**: instead of a high-level summary, the LLM can retrieve an exact sentence you said months ago if it’s strongly relevant to your current query. This can greatly improve factual accuracy and continuity, since the LLM sees the *exact reference* rather than relying on its parametric memory or a paraphrased summary. The approach is also **scalable**: vector search is fast even for large databases (sub-linear or indexed search), and new entries can be added incrementally without reprocessing the entire memory.

**Challenges:** A key challenge is ensuring relevant memories are retrieved. Semantic search isn’t perfect – if the user’s query is phrased very differently from how the memory was stored, the similarity may be missed. For instance, the user might ask, “Did I ever mention a pet?” and the memory store has “My dog Fido loves playing fetch.” If the embeddings are good, “pet” vs “dog” should still match, but there’s some chance of missing the connection if the phrasing diverges. We can mitigate this by using high-quality embedding models (OpenAI’s ada v2, etc., or domain-tuned embeddings) and by storing **multiple representations** (maybe the full text and some keywords). Another issue is handling **context and co-occurrence**: a single vector chunk might not capture the broader context (e.g., a decision made over several messages). Retrieving one snippet could lack context; retrieving many increases token usage. Solutions include chunking conversations into coherent pieces and maybe retrieving a few and letting the LLM infer the rest via its own reasoning.

There’s also the **volume** concern: if you store *everything* as vectors, over time you could end up with thousands of embeddings. While vector search can handle a lot, irrelevant or low-value items might clutter the results or slightly slow down retrieval. Thus, many systems combine this with some pruning or filtering – for example, only embedding *important* messages or using time as a factor (more recent memories might be boosted, older ones considered only if highly relevant). The Generative Agents framework by Park et al. addressed this by scoring memories based on recency, importance, and relevance to the current situation. In their agent architecture, each memory had an importance score (possibly assigned by an LLM reflecting on how significant that event was) and the retrieval function combined that with a recency decay and embedding similarity to choose what to pull into the context. This kind of weighted retrieval ensures the agent doesn’t fixate on an obscure detail from long ago at the expense of something more pertinent. Our design can adopt a similar strategy (see Proposed Design section), effectively merging semantic similarity with meta-data like timestamps and manually or automatically set importance flags.

In summary, vector-based memory is **powerful for precise recall** and is a key component of many long-term memory systems. It is promising when paired with good relevance filtering. The limitation is that it requires an embedding model and introduces an extra retrieval step (which adds slight latency and complexity). It also works best in tandem with other approaches: e.g., combine raw snippet retrieval with summaries or knowledge extraction, to cover both exact recall and general context.

### 3. **Hybrid and Hierarchical Memory Systems**

The most **promising designs use a hybrid approach**, taking inspiration from how humans have multiple memory systems (short-term vs long-term, episodic vs semantic). Several notable attempts fall into this category, combining immediate context, vector recall, structured knowledge, and summarization:

* **MemGPT (Memory as an OS)** – This research (2023) conceptualized an LLM’s memory as **multi-tier storage**, analogous to a computer’s memory hierarchy. MemGPT defines four memory tiers:

  * **Core Memory:** The essential system context or persona that is always provided (e.g. instructions, identity). This is akin to a stable long-term memory that defines the AI’s role or the user’s basic profile and never leaves the context window.
  * **Message Memory:** A rolling buffer of recent dialogue (the last several messages that still fit in the prompt). This is like short-term memory or “RAM”, containing the most recent exchanges verbatim.
  * **Archival Memory:** The long-term store (e.g. a vector database of older conversations or notes) where less recent information is kept available outside the prompt. This is comparable to a hard drive – a complete but offline store.
  * **Recall Memory:** A mechanism to fetch relevant items from Archival Memory into the active context when needed. In the MemGPT model, when the user input arrives, the system may offload the oldest messages to archival storage if the context window is full, then load the core memory, the recent messages, and perform a search on the archival memory for any relevant content, pulling those into the prompt as **retrieved memories**. All these memory streams (core, recent, retrieved) are then combined for the LLM to process, after which the new interaction is stored into message memory and possibly archived if needed.

  This design is powerful because it explicitly manages **what stays in immediate context vs what is stored externally**, and uses the LLM itself (or a controller around it) to decide when to swap things in or out (much like virtual memory paging). MemGPT effectively **extends context length** by this swap-and-retrieve approach, demonstrated with different priority levels for memories. The limitation is that it’s a complex orchestration – one needs to tune the triggers for retrieval and archiving. If done poorly, the model might archive something still needed or fail to recall something important. But as a conceptual framework, MemGPT’s tiers ensure both **continuity** (core + recent context always present) and **coverage** (older info not lost but on-call). Our proposed scaffolding will incorporate this multi-tier idea: distinguishing short-term context, long-term vector store, and a persistent profile memory.

* **Generative Agents (Memory Stream + Reflection)** – In the *Generative Agents* work, AI agents (simulated characters) have a **memory stream** that records every observation or interaction as a *memory object*, tagged with time. The agent uses a **retrieval function** that scores these memory objects by relevance to the current situation, recency, and importance; the top memories that fit the context window are inserted into the prompt to influence the agent’s next action. Crucially, the system also includes a **Reflections** module. Over time, the agent periodically pauses to **reflect** on accumulated memories, distilling them into higher-level **insights** or **conclusions**. For example, after many interactions, an agent might reflect: “I’ve noticed Bob avoids talking about his job, he might be unhappy with it.” These reflections are stored as part of memory and are retrievable in the same way, giving the agent more abstract knowledge to guide its behavior. There’s also a Planning module, but for memory purposes the key idea is that raw experiences can be processed into summaries or generalizations that are easier to use. This is analogous to human memory consolidation – we extract lessons or stable traits from specific events.

  The generative agent approach validates the **scoring retrieval** method (combining recency, importance, embedding similarity) and shows the benefit of **periodic summarization/reflection** to avoid overwhelming the memory with minutiae while preserving essential learnings. One can imagine our LLM friend agent, after many chats, reflecting (via an LLM call in the background) to update the user’s profile: e.g. “The user has consistently mentioned stress about work; they value mindfulness and might appreciate gentle check-ins.” Such a reflection could then be stored as a succinct piece of semantic memory that informs future responses (like showing more empathy or recalling to ask about work stress later). The downside is the cost/complexity of doing these reflections – one must schedule them at appropriate intervals and avoid the model generating false inferences. But it’s a powerful tool for **scaling memory**: rather than storing 100 disconnected facts, the agent might summarize them into a theme that’s easier to remember.

* **Structured Knowledge Bases (Knowledge Graphs)** – Some systems attempt to store facts from the conversation in a **structured form** (triples or key-values) in addition to raw text. For instance, LangChain’s long-term memory example extracts information about entities in the form of `(subject, predicate, object)` triples. If a user says, “I work as a graphic designer at ABC Corp,” the system might extract a triple like `(User, occupation, graphic designer at ABC Corp)` and store that in a small database. Over time, this builds a profile of the user (and possibly other entities mentioned) in a **graph-like memory**. When the LLM needs to answer a question such as “Where do I work?” or something that involves known facts (“Can you remind me what my job is?”), the system can directly query this structured store. This has the advantage of **precision** – unlike vector search, which might retrieve a whole sentence or require the model to interpret it, the knowledge graph can directly yield the answer or a consistent piece of info. It’s also easier to **update** discrete facts (if the user changes jobs, update that node without retraining or searching). MemoryBank’s *user portraits* are conceptually similar: the system builds a profile of personality traits and preferences, which is effectively structured semantic memory about the user.

  The challenge with this approach is it requires an extra NLP layer (information extraction) which might miss or misinterpret things. It’s easier to do for clearly structured info (names, dates, relationships) and harder for nuanced things (“the user feels anxious about X” – is that a fact to store? It could be, if phrased carefully). Also, maintaining consistency in the knowledge base is crucial – conflicting info should be resolved (only one current job, for example) which may need explicit logic. Despite these challenges, integrating a knowledge store for core facts greatly helps an LLM companion avoid embarrassing lapses like forgetting a user’s name or recommending a food they are allergic to. Our design will include a **User Profile Memory** component serving this role.

* **MemoryBank (Zhong et al. 2023)** – This approach deserves special mention as it combines many of the above ideas into a cohesive system. MemoryBank’s architecture has:

  * **Memory Storage**: It logs all interactions (with timestamps) for completeness, but also maintains **hierarchical summaries of dialogues** to condense day-to-day conversations. Additionally, it creates **dynamic user profiles (“user portraits”)** capturing the user’s personality traits and preferences over time. In short, it separates raw episodic memory (the log), distilled episodic memory (summaries of events by day or topic) and semantic memory (profile of the user).
  * **Memory Retrieval**: When the LLM needs to respond, MemoryBank encodes the current context into a vector and performs an **embedding-based search** over the stored memories. Because it has summaries and raw logs indexed, it can retrieve either high-level events or specific past details, as appropriate. This is essentially RAG as described above, using tools like FAISS for efficient similarity search. The relevant memory items are then **integrated into the prompt**, providing personalized context for generation.
  * **Memory Updating**: MemoryBank implements a **forgetting and reinforcement mechanism** inspired by human memory theories. Memories have a “strength” that decays over time if not used (simulating Ebbinghaus’ curve). If a piece of information hasn’t been brought up for a long time, the system might gradually lower its priority or even archive it out of active circulation (it could be moved to a deep archive or deleted) – this prevents the memory store from growing without bound with low-value data. Conversely, each time a memory is retrieved or referenced, it gets “boosted” and its decay is reset, so important facts stick around. This way, the system **focuses on what matters to the user**, staying relevant. The memory updates also continuously refresh the **summaries and user portrait**. For example, if the user’s preferences change or new themes emerge, the profile is adjusted to reflect that current reality. This adaptive profiling ensures the LLM’s understanding of the user isn’t stale.

  MemoryBank was demonstrated with *SiliconFriend*, a long-term AI companion chatbot. SiliconFriend showed improved ability to recall past conversations and personalize responses to different users’ personalities. Essentially, it validated that combining logging, summarization, profile-building, vector retrieval, and human-like forgetting creates a more **resilient long-term memory** for chatbots. The complexity of MemoryBank is higher than simpler methods, but it’s a template for what a robust LLM memory system can look like.

**Takeaways:** The cutting-edge attempts suggest that **no single method is sufficient**. Pure summarization loses detail, pure vector search might surface irrelevant bits without understanding context, and storing everything forever creates clutter. The best solutions **hybridize** these approaches, much like the human brain’s multi-faceted memory. They maintain different **types of memories** (recent transcripts, long-term facts, summary of experiences, personal profile) and use a **retrieval strategy** that picks the right memory for the right occasion. They also implement **memory management** (deciding what to keep salient and what to archive or forget). This informs our design: we will propose a scaffold with distinct components for short-term context, long-term vector store, and user profile, along with algorithms for *memory retrieval* and *memory update* (prune/reinforce) that ensure the system remains both effective and efficient over time.

## Proposed Long-Term Memory Scaffolding Design

Using the lessons from human memory and existing LLM memory systems, we propose a **hybrid long-term memory architecture** for an LLM-based AI companion. This design is model-agnostic and can work with any LLM that allows additional context (via prompt or an API with function-calling capabilities). All memory data is stored locally; the only calls to the LLM are for generating responses or performing occasional summarization/extraction. We outline the key components and processes below, followed by a schematic flow of how the system operates:

### **Memory Components**

Our system will maintain **three interconnected memory stores**, each optimized for different types of information:

* **1. Short-Term Conversation Buffer (ST Memory):** This is the running window of recent messages that stays in the LLM’s prompt (like MemGPT’s message memory). It might simply be the last *N* user and assistant messages that fit within the LLM’s token limit (or last N turns if fixed). This buffer ensures continuity for immediate context (“what were we just talking about?”). It’s analogous to human working memory or the content you actively have in mind during a conversation. This component already exists by default in chat interfaces (sliding context window), but we explicitly manage it by possibly truncating older turns when the buffer is full, *after* archiving them to long-term storage. The short-term memory doesn’t need special storage beyond being in the conversation history; it resets with each new session (though important bits should persist via long-term memory).

* **2. Long-Term Vector Store (LT Memory):** This is a local database (could be a simple JSON with embedded vectors, a SQLite with vector extension, or an in-memory FAISS index, etc.) that holds **embeddings of past interactions or notes**. Each entry in this store consists of:

  * a **memory content** (text of a user utterance, assistant response, or a summary/note),
  * associated **metadata** (timestamp, message author, maybe a tag for topic),
  * an **embedding vector** representing the semantic content of that memory content.

  This forms the bulk of the AI’s episodic memory – a record of conversations and facts gleaned from them. Critically, we do **not rely on the LLM’s internal weights to recall this data**; instead, we explicitly retrieve relevant pieces when needed (non-parametric memory). The vector store can be persisted on disk for durability across sessions. Only the *embedding model* needs to be chosen – for cost efficiency, one could use an open-source embedding model running locally or a cheaper embedding API. The vector dimension depends on model (e.g. 1536-d for OpenAI Ada). Since everything is local, privacy is preserved; only the user’s LLM queries and the final assembled prompt (which may include some memory snippets) go to the LLM API.

* **3. Structured User Profile (Semantic Memory):** This is a lightweight knowledge base storing key **facts, preferences, and attributes** about the user (and possibly other entities regularly discussed). It can be implemented as a set of triples or key-value pairs in a local store. For example, it might contain:

  * `{Name: "Alice"}`
  * `{Occupation: "graphic designer"}`
  * `{FavoriteCuisine: "Italian"}`
  * `{Allergies: ["peanuts"]}`
  * `{Pet: "dog named Fido"}`
  * `{FrequentTopics: ["classical music", "hiking"]}`
  * `{PersonalitySummary: "Often anxious about work, appreciates humor"}`

  Think of this as the LLM’s knowledge graph or **long-term semantic memory** of the user. It’s akin to MemoryBank’s user portrait. The profile is derived from the conversations (initially maybe an empty profile that fills over time) and updated as new information comes in. This store enables direct recall of facts without needing a fuzzy search each time – if the user explicitly asks *"What’s my job?"*, the system can look up the `Occupation` field. Even when not asked directly, the profile can inject crucial details: for instance, always remind the model of the user’s name, or caution the model away from suggesting peanut recipes. The profile storage could be a simple JSON or Python dict in memory, or a tiny SQLite table; scale is small since it’s distilled info.

  We also include the **AI’s persona** in this category. If the LLM agent itself has certain fixed traits (e.g. its name, its role as a friend, speaking style), that can be stored as part of core memory and included in the system prompt every time. This ensures consistency in how the AI behaves – effectively the AI’s own profile. In MemGPT terms, this is the *Core Memory* that is always loaded. It might be static (written by developers) or slightly adjustable (the AI notes that it should adopt a certain tone the user likes, etc.).

These three stores correspond loosely to **episodic short-term**, **episodic long-term**, and **semantic long-term** memory respectively, mirroring human memory divisions. They are stored locally and updated continuously.

### **Memory Update Pipeline**

To keep the memory up-to-date, we establish a pipeline that triggers on each new user input (and possibly after the assistant responds). Here’s how new information flows into the memory system:

1. **Capture New Interaction:** Whenever the user sends a message, that message (and potentially the assistant’s last reply if not already stored) is logged. We create a time-stamped entry of this interaction. This ensures **persistent logging** of everything (which is important for traceability and if we ever need to rebuild a summary).

2. **Embed and Store:** The new user message (and any significant assistant answer) is converted into an embedding vector via the chosen model. We then store this vector along with the text into the **Long-Term Vector Store**. This happens asynchronously or in the background if possible, to not delay the response. It’s a quick operation (embedding APIs are fast, and local models can embed short text quickly). By doing this immediately, we ensure even the latest turn is searchable for future queries. If the conversation is multi-turn, we might hold off embedding every single message to save cost (maybe embed only user messages or full QA pairs rather than every assistant response), but for completeness, embedding both user queries and assistant answers can help if the user later references “what you told me about X”.

3. **Semantic Extraction:** We analyze the new content for any **facts or preferences** that should update the **User Profile**. This can be done by a small LLM call or a rule-based system. For example, if the user says “I just adopted a kitten named Whiskers!”, the system should extract `[Pet: "kitten named Whiskers"]` and add it to the profile (or update the existing Pet entry). If the user corrects something (“Actually, I moved to New York last month”), update the profile field for location. One efficient way: maintain a list of profile fields of interest and regex or ML-detect mentions of them (e.g. look for patterns like “I am a \*”, “I have a \*”, “I like \*”). For more complex traits (like mood or personality), it might require a periodic summary rather than real-time extraction. But at least for concrete facts, updating the profile ensures that information can be recalled exactly later without search.

   We also watch for **user-marked important information**. If the user says “Please remember that my mother’s name is Carol,” we flag that as high importance (and perhaps immediately commit it to profile: `MotherName: Carol`). Even if the user doesn’t explicitly say “remember this”, we can heuristically treat certain categories as important (names, preferences, tasks, etc. are likely important).

4. **Summarization (Periodic):** Rather than summarizing every turn (which is costly and could accumulate errors), we adopt a **periodic or triggered summarization** strategy. For instance:

   * **Session-end summarization:** When a conversation session ends (or every day), the system can prompt the LLM to summarize the key points of that session. This summary is then stored in the vector DB (and possibly also in a “daily summary” list). For example, “On 2025-07-11, user discussed their new project at work and expressed anxiety about the deadline. Also talked about weekend hiking plans.” This provides a condensed memory of that session for future recall without scanning every message.
   * **Long conversation chunk summarization:** If a single conversation is getting very long (hundreds of turns), we might summarize older parts on the fly. E.g., after every 50 turns, summarize and replace those 50 with a summary note (keeping the raw logs archived elsewhere if needed). This is akin to hierarchical summarization used in Pieces.app. The summary would also be embedded into the vector store.
   * **Reflection-based summarization:** Optionally, as done in generative agents, the system can sometimes run a “reflection” prompt: “Given all interactions from the past week, what are the user’s main concerns or goals?” – then store that answer as a meta-memory. This helps the AI form higher-level concepts (like “user is bored with their job lately”) that might not be explicitly stated but are gleaned from patterns. This would be done sparingly (maybe weekly or when a clear pattern emerges) because it costs tokens and risks introducing inaccuracies. However, if done carefully, it aligns the AI’s long-term understanding closer to the user’s reality and improves personalization.

   All summaries themselves will be stored with proper markings (like a tag “summary” and date range covered) in the vector store. This way, when searching memory, a query might hit either a raw detail or a summary. Often, retrieving a summary might be sufficient to answer broad questions like “How have I been feeling about work lately?” whereas a specific question might retrieve the exact conversation snippet.

5. **Forgetting/Pruning:** To keep the system efficient and the memory relevant, we implement a **pruning mechanism**. This could be time-based and importance-based:

   * If a memory entry (especially a raw message) is, say, over X months old and has never been retrieved or referenced since, we can archive it to a deeper storage (e.g., move to a compressed archive file or delete it after summarization). Important information should not be pruned; only low-significance items are. We might rely on an “importance” score attached to each memory. By default, direct user profile facts and anything the user highlighted as important get a high score (never delete). For other entries, we can assign a score based on message content (maybe an LLM can rate importance 1-10 each time it summarizes or on the fly for each new entry, similar to Park’s generative agent assigning importance to observations).
   * We also simulate forgetting curve by decaying an implicit score for each memory. Each time a memory is retrieved, boost its score. If it remains untouched, decay over time. When it falls below a threshold (and is old), consider it for removal. This ensures the memory database doesn’t grow indefinitely with clutter, and speeds up retrieval by focusing on memories that have stood the test of time or use.

   In practice, “removal” might just mean moving it out of the actively searched index. We could keep a backup store of pruned items in case, but ideally, important stuff won’t be pruned due to either being summarized or referenced at least once in summary.

6. **Core Persona Consistency:** We maintain the AI’s core instructions (and crucial user facts) in a static prompt or file. This isn’t updated every turn, but the developer might refine it or the AI might append notes to it over time. For example, if the user says “I prefer you be more sarcastic,” the system could update the AI’s persona settings to adopt a witty tone with the user. Generally, though, persona memory is simpler than user memory.

All these updates happen locally. The cost incurred is mainly in occasional LLM calls for summarization or extraction and storage overhead for vectors. We aim to minimize calls by batching operations (e.g. do one summary for a whole session, not every message) and possibly using smaller models for certain tasks (for instance, a local model fine-tuned for summarization or entity extraction could reduce API calls). The result is that the memory stays **fresh and reflective of the user’s current state** without manual intervention.

### **Memory Retrieval & Usage**

With memory stored and maintained, the crucial part is **using it in real time** to help answer user queries or continue conversations smoothly. Here’s how the retrieval process works when the LLM needs to generate a response:

1. **Determine When to Retrieve:** In principle, we could retrieve memories for every user query. However, to reduce unnecessary work, we can decide retrieval triggers. For example, if the user’s message is very simple or obviously unrelated to personal context (“Tell me a joke” might not need memory), the system could skip memory retrieval and just use the prompt. But if there’s any chance that personal or past context is relevant – and in a personal AI companion scenario, *most* messages will have some context – we perform retrieval. We may err on the side of retrieving, since the cost of a vector search is small and adding a bit of context usually doesn’t hurt (the LLM can ignore irrelevant info if not needed). If the LLM supports function calling or tool use, we could also let the LLM *decide* by asking it: “Do you need to recall something?” but that adds complexity. A simpler approach is an automatic retrieval on each turn, possibly with some basic filtering.

2. **Formulate the Search Query:** We take the current conversational context as a cue for memory. A strong approach is to use the **last user message** (and maybe the AI’s last reply for context) as the query to the vector store. We encode this text into an embedding vector the same way we did for stored memories. Then we perform a similarity search in the **Long-Term Vector Store**. This yields, say, the top *k* memories that are semantically related to the user’s query or statement.

   We might also perform targeted searches for specific information if needed. For example:

   * If the user’s query itself looks like a direct question about past events (“When did I last go hiking?” or “Do you remember what my score was in that game?”), we might parse that and search the memory specifically for “hiking” or that game, perhaps even using keyword search on metadata in addition to embedding search.
   * We can utilize the **metadata**: e.g., prefer results where the same topic was mentioned, or use time filtering if the query says “last week” (search in memories around last week).
   * Additionally, check the **User Profile store** for any direct matches. If the user asks a factual question about themselves (“What’s my favorite cuisine?”), we directly look up the profile entry rather than relying on semantic similarity. In fact, we can treat the profile as part of the retrieval results: simply convert each profile fact into a sentence (e.g. “Your favorite cuisine is Italian.”) and embed those too or include them at high priority if relevant. Because profile facts are small and highly relevant, we might just always include certain critical ones (like name, important health info) in a system prompt.

3. **Rank/Filter Retrieved Memories:** The raw similarity search gives us candidates. We should apply the recency/importance heuristic here: for example, if two memory snippets have similar relevance scores, give preference to the more recent one (if the user is asking “Did I enjoy the movie we saw?”, a memory from last week about a movie is more likely relevant than a similar mention two years ago – unless it specifically was flagged important). We can also limit to a reasonable number of memory snippets to avoid overloading the prompt (perhaps 3 to 5). Each snippet might be a few sentences or a short paragraph, ideally not more than, say, 50 tokens each. If any snippet is too long, consider truncating or summarizing it before insertion. In many cases, the stored entries themselves may already be summaries if from older sessions, which keeps them concise.

   Additionally, ensure diversity of information: if all top results are about the same event, we might not need all of them. If the query is broad (e.g. “How have I progressed in fitness?”), we might retrieve one memory from each relevant time period or category (some from early discussions of fitness, some recent). This can be handled by clustering or by the nature of how we store summary vs detail.

4. **Compose the Prompt with Memories:** We then construct the LLM’s input to include the retrieved context. A safe format is to prepend a **system message** that provides these memories as notes. For example:

   ```
   [System message to LLM:]
   You are a helpful AI friend with access to the user's shared memories. Here are some relevant memories:
   - Memory (2025-06-10): You and the user discussed their new workout routine. The user said they started running 3 times a week and felt great afterwards.
   - Memory (2025-07-01): The user mentioned they were struggling to maintain their exercise schedule due to work stress.
   - Memory (Profile): The user’s goal is to run a marathon this year.
   Use these memories to inform your answer.
   [End of system message]

   [User message]: "How have I been doing with my fitness goals lately?"
   ```

   The system message conveys the memories in natural language that the LLM can understand and incorporate. We clearly mark them as past info so the LLM doesn’t treat them as new user input but rather context. This way, the LLM has both the user’s latest question and the background to answer it. In the above case, the LLM could then answer summarizing the user’s fitness progress based on those memories.

   If using an API like OpenAI’s, we’d supply something like: `messages=[{"role": "system", "content": memory_context}, {"role": "user", "content": user_message}]`. For open-source models, we’d prepend it in the prompt.

   Importantly, we **do not exceed context limits**. If the LLM has a 4k token limit, and our short-term conversation plus memories plus prompt instructions approach that, we may need to reduce *k* or the length of each memory. The hierarchical storage helps here: a high-level summary memory might cover a lot in few tokens. If a model with a larger context (like 16k or 100k) is used, we can include more, but we still try to keep only *relevant* info rather than dumping the whole history.

5. **LLM Generates Answer:** The LLM, now armed with both the recent dialogue and pertinent long-term memories, composes a response. Ideally, it will seamlessly integrate the recalled info (“I know it’s been hard for you to run regularly since work got busy, but remember how great you felt when you were consistent in June...”). The user experiences an answer that shows the AI remembers past discussions and personal details, increasing the sense of continuity and usefulness.

6. **Post-Answer Memory Storage:** After the assistant responds, we consider if the *assistant’s answer itself* should be stored. If the answer contains information that isn’t directly from the user (maybe some advice or a plan the AI gave), it might be worth embedding and storing as well – because the user might later refer to “the plan you gave me” or the AI might need to recall its own suggestions. Many memory systems do store both sides of the conversation for completeness. We would likely do the same: after finalizing the answer, embed it and store in vector DB with a tag “assistant said this at time T”. This ensures the memory is symmetrical.

   Also, if the assistant’s answer led to some new conclusion about the user (for example, the assistant deduced something and that should be part of the user profile), we could update that. But generally, new facts come from user, not the AI.

7. **Repeat for Next Turn:** On the next user message, the process repeats, continuously incorporating new inputs and leveraging stored memories.

### **System Efficiency and Scalability**

This design is mindful of **cost and speed**:

* **Local Operations:** All heavy memory operations (storing, searching, updating) are local and use lightweight libraries. Vector searches are very fast (milliseconds) even for thousands of entries, especially if using approximate nearest neighbor indexing. Profile lookups are trivial in time. So any overhead comes only from additional LLM calls we trigger.

* **Minimizing LLM Calls:** We primarily call the LLM for generating the response. Additional calls include occasional *summarization* or *extraction*. These can be throttled (e.g., summary only after a long session, not every time) to maybe one extra call per dozens of user turns. Even that could be done with a cheaper model if absolute cost is critical. If using the same LLM for summarization, one might do it during off-peak times or when the conversation is idle. Also, embedding APIs (like OpenAI’s) have a cost, but considerably smaller compared to chat completion. Alternatively, open-source embedding models can run on local GPU/CPU at no cost.

* **Memory Pruning:** By removing stale data, we prevent the vector store from slowing down or consuming too much memory/disk over the long run. The forgetting mechanism also means the system’s retrieval remains snappy and relevant.

* **Real-Time Response:** The retrieval+prompt assembly process is fast enough that the user shouldn’t experience a significant lag. For example, a vector search (few ms) + adding e.g. 3 memory snippets (negligible time) might add only, say, 100ms overhead before the LLM is called. If summarization isn't happening in that loop, the main delay is just the LLM responding, which is unchanged. Even if we did a quick extraction call, that’s one extra roundtrip – still likely under a couple seconds total. The user mentioned even a few minutes is acceptable for complex recall, but our aim is closer to real-time (a few seconds). This is **analogous to a human**: a person usually answers from memory immediately, but if it’s a hard recall, they might say “Give me a second to remember…”. We could simulate that if needed (the assistant could have a slight delay or mention it's thinking), but ideally it’s not noticeable.

* **Scalability:** The design can handle a growing amount of knowledge gracefully by:

  * Summarizing and compressing older information (so the number of items grows sub-linearly with time).
  * Forgetting low-value data.
  * Partitioning memory by subject or time to speed up searches (for instance, we could maintain separate vector indices per quarter or per topic cluster, and search only the relevant one based on context).
  * Working with more capable models if available (if you have a 100k-token context model, you might load larger chunks of memory directly; but our design doesn’t require that).
  * Accommodating improvements: if a new embedding model is better, we can re-embed data in background; if a new summarization technique is safer, we can apply it to existing logs.

### **Security and Privacy:**

All user data remains local in this approach. The memory files (vector DB, profile) can be encrypted or at least stored in user’s space. When sending to the LLM (if using a cloud API), we do include some memory snippets in the prompt, so strictly speaking those pieces of data do leave the local environment. Users should be aware of that in terms of privacy – e.g., if using OpenAI API, the snippets sent might contain personal info. One might choose an open-source local LLM for full privacy, at cost of model quality. It’s a trade-off scenario. But since the user specifically noted local storage is required (which we do), I’ll note that caution.

The system should also avoid sending overly large or unnecessary personal data to the LLM. Only relevant snippets are sent, reducing exposure. And if the user has some data they never want repeated to the LLM (maybe extremely sensitive info), one could tag it in memory to never auto-retrieve unless user explicitly asks. That’s a refinement to consider.

### **Example Scenario**

To illustrate the design in action, consider a simplified scenario:

* **User and AI have been chatting for months.** The profile store says: `Name=John; Allergies=[peanuts]; FavoriteBand=The Beatles; Pet=dog named Fido; Stressors=[work deadline]; Goal=get fit`. The vector store contains numerous embedded memories, including a summary of last week’s talk about a work deadline and a snippet where John mentioned he felt *“really down after that meeting with my boss”*.

* **Today, User asks:** *“I’m feeling off… maybe we talked about this before. What do I usually do when I feel this way?”*.

  * The system takes this query, embeds it. It finds relevant vectors: one from two months ago where John discussed coping with stress (e.g., *“I usually go for a walk or play guitar when I feel low”*), and another from last week’s summary about the boss meeting and that the AI suggested taking a break.
  * It also recognizes the user is asking about *what they usually do* when feeling down – this implies something about habits. Perhaps the profile has nothing explicitly on “coping habits”, but the memory snippet does.
  * The system retrieves these two memory items. It also grabs from profile that John has a dog (maybe relevant if walking dog was mentioned as a mood booster) – it decides to include that too since exercise with Fido might have come up.
  * The prompt is composed with a system message:

    > Memories: (1) *A few months ago, you told the AI: “When I feel down, I often go for a walk or play my guitar to clear my head.”* (2) *Last week after a tough meeting, the AI suggested you take a break and it seemed to help.*
    > User asks: "I'm feeling off...what do I usually do when I feel this way?"
  * The LLM responds: *“John, I remember you’ve mentioned that when you’re feeling down, you often take a walk or play your guitar. For instance, after that rough meeting last week, you took a short break. Maybe try going for a walk with Fido or strumming a few songs. Those have helped you in the past.”*
  * The answer is rich with personal context, which would not be possible without the memory system.

* **After responding**, the AI logs the user’s query and its answer, embeds them, and stores them. If “feeling off” was a new context, maybe it adds a note to profile like `CopingStrategies: ["walk", "music"]` for quicker reference next time. If this pattern continues, the memory system will strengthen those associations (because we keep retrieving that snippet, reinforcing it per the algorithm).

This example demonstrates how the scaffolding achieves the goal: the AI behaves like a close friend who remembers past conversations and personal details **without retraining the model** – all through external scaffolding and clever use of storage and retrieval.

## Conclusion and Future Directions

We have proposed a comprehensive long-term memory scaffolding for LLMs that draws on cognitive principles and proven techniques from recent research. In summary, our design: (a) stores dialogue history and facts locally, (b) uses semantic vector search to retrieve relevant memories in real-time, (c) maintains a structured profile of the user for exact recall of important details, (d) uses summarization to condense and understand long histories, and (e) employs a forgetting mechanism to remain efficient and focused over time. All of this is done external to the model, making it model-agnostic and cost-efficient – we leverage existing LLMs via their APIs, and avoid expensive long context windows or fine-tuning for each user.

This scaffolding should enable an LLM to **remember a user’s life** in a meaningful way: never forgetting frequently mentioned facts, learning from ongoing interactions, and providing context-aware responses that evolve as the user’s story evolves. The AI will pick up exactly where it left off, even if you pause a conversation for days or switch topics, because the important pieces of past context are always available to it when needed. Essentially, we’ve turned the stateless LLM into a stateful companion, with the state managed in a principled manner outside the model.

**Future improvements** might explore letting the LLM itself control memory more (via tool use, e.g. calling a “SAVE\_TO\_MEMORY” function when it judges something worth remembering). This could offload some decision-making to the model – for instance, it might notice “the user just gave me their new phone number, I should store that.” Another area is experimenting with **spaced repetition** prompts: occasionally quiz the LLM on a known fact to reinforce it (though since our LLM doesn’t actually update weights, this would be more to ensure the info stays accessible in vector memory by linking it to recent context). Additionally, integration with scheduling: e.g., if the user sets a goal or date, memory could prompt the AI to remind the user later (blending into planning agents). All these are built on top of a reliable memory foundation.

By combining insights from human cognition with current AI tools, the proposed design offers a **scalable and realistic path** to give LLMs long-term memory. It acknowledges that true long-term retention isn’t solved by raw model capacity alone, but by a supportive system that stores, curates, and provides information when appropriate. With this scaffolding, an LLM agent can truly act like a long-term companion that “remembers when” – enhancing user experience in applications from personal AI friends to therapy bots, tutors, and virtual assistants that grow and learn with their users over time.

By implementing this design, developers can unlock an **extended memory** for any LLM (open-source or API-based) at a fraction of the cost of training giant context windows or specialized models. The LLM remains stateless in itself but effectively gains state through our architecture. This modular approach also means each component can be improved independently: better embedding models can drop in, new summarization algorithms can replace old ones, etc., continuously improving the quality of the AI’s memory. Ultimately, bridging the gap between how humans remember and how LLMs operate will bring more natural, coherent, and **personalized** interactions – making AI not just a smart responder, but a truly **recollecting partner** in dialogue.

**Sources:**

* Insights on generative agent memory architecture
* MemGPT multi-tier memory concept
* MemoryBank approach to long-term LLM memory
* Hierarchical summarization for long-term context
* Human memory encoding and retrieval mechanisms
* LangChain long-term memory example
